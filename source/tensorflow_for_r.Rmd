---
title: "Tensorflow for R"
output: github_document
---

The aim is to go through some of the basics of using `tensorflow` for auto-differentiation.

```{r}
# Just some setup
library(dplyr)
```

### Installation

I have been able to get everything to work via using `conda` and the standard install script. The official page for this is found here[https://tensorflow.rstudio.com/installation/].

Note: When using the installation with `conda`, I had some major issues when it was not downloaded into the standard user location, i.e., `/home/<user>/anaconda3`.

What I did was:
```{r}
install.packages("tensorflow")
library(tensorflow)
install_tensorflow("conda")

# Also included and useful
library(keras)
```

A new conda environment will be made for tensorflow for R: `r-reticulate`. You should already have this if you are using the `reticulate` package.

### Basic Constant/Variable

There seens to be two main datatypes:
 - The constant `tf$constant`;
 - The variable `tf$Variable`.
 
These are methods for defining tensors. We can define tensors simply by passing R lists:
 
```{r}
r_list <- list(c(1, 2, 3), c(4, 5, 6))
const_tensor <- tf$constant(r_list)
var_tensor <- tf$Variable(r_list)

print(const_tensor)
print(var_tensor)
```

One of the nice inherited properties from Python is the indexing notation.

```{r}
print(const_tensor[, `::-2`])
print(var_tensor[, `::-2`])
```

For non-R-native indexing notation, the "tick"s are required. Also notice that the resulting tensors are of the constant type.
Full information can be found here[https://tensorflow.rstudio.com/guide/tensorflow/tensors/].
 
There are some important differences for the two types:
 
1. I haven't been able to find a method of doing assignment for constant tensors (as per its name-sake). However, we can do this for variables:

```{r}
print(var_tensor)
var_tensor[1, 2]$assign(-1)
print(var_tensor)
```

One work around for constant vectors is the use of indicator functions/one-hot-vectors:

```{r}
indicators_1 <- tf$one_hot(as.integer(c(0, 1)), as.integer(2))
indicators_2 <- tf$one_hot(as.integer(c(0, 1, 2)), as.integer(3))
idx_1 <- tf$reshape(indicators_1[1, ], shape(1, -1))
idx_2 <- tf$reshape(indicators_2[2, ], shape(1, -1))

position_indicator <- tf$transpose(idx_1) * idx_2
print(position_indicator)
const_tensor <- const_tensor + position_indicator * ((-1) - const_tensor[1, 2])
print(const_tensor)
```

NOTE: that in `tf$one_hot` we switch from R indexing (start from 1) to Python index. This happens for some of the functions in `tf$<function>`. Check the **Python** tensorflow documentation when working with these: here[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/one_hot].

Please let me know if you find a better way!

2. In variable tensors, we cannot use assign when calculating gradients! This one is an important one. We will go through automatic differentiation immediately below now.

### Automatic Differentiation

There are a few important functions which we need:
  - `tf$GradientTape()`: keeps track of computation and calculate gradients;
  - `t$watch(x)` where `t` is the tape: informs the tape to keep track of computation for constant tensor `x`.
  
Here is a simple example:

```{r}
f1 <- function(x, y) {
  acc <- 0
  for (i in 1:length(y)) {
    acc <- x[1] * y[i] + log(x[2] * y[i])
  }
  return(acc)
}

grad.f1 <- function(x, y) {
  with(tf$GradientTape() %as% t, {
    t$watch(x)
    val <-f1(x, y)
  })
  return(t$gradient(val, x))
}

input <- c(5, 2)
y_input <- c(2, 3, 4, 5)

print(f1(input, y_input))
print(grad.f1(tf$Variable(input), y_input))
print(grad.f1(tf$constant(input), y_input))
```

```{r error=TRUE}
print(grad.f1(input, y_input))  # Notice this gives an error
```

Note that the result will be a tensor of constant type, and by default is `tf$float32`.
Also note that when computing the gradient, tensorflow becomes unhappy when our input is not in a tensorflow format.
We can make a quick wrapper around the gradient function as follows:

```{r}
r_grad.f1 <- function(x, y) {
  x <- tf$convert_to_tensor(x)
  return(grad.f1(x, y))
}

r_grad_r.f1 <- function(x, y) {
  x <- tf$convert_to_tensor(x)
  return(grad.f1(x, y)$numpy())
}


print(r_grad.f1(input, y_input))  # Now it is happy :)
print(r_grad_r.f1(input, y_input))  # Converts back to R for demonstration below
```

Lets compare the gradients now:

```{r}
library(numDeriv)
library(nloptr)

print(grad(function(x) f1(x, 2), input))
check.derivatives(.x = input, func = function(x) f1(x, 2), func_grad = function(x) r_grad_r.f1(x, 2))
```

Pre-good right? Well lets try a different function.
Here are the inputs:

```{r}
x_input <- c(1, 2, 3.5, 1, 2)
y_input <- c(1.5, 2.2, 3, 1, 4)
t <- 2
```

Here are the outputs:

```{r}
f2 <- function(t, xs, ys) {
  acc <- tf$zeros_like(xs)
  for (i in 1:length(xs)) {
    mask <- tf$cast(ys < t, tf$float32)
    acc <- acc + (t - xs[i] * log(t)) * mask
  }
  return(acc %>% k_sum)
}

f2_t <- function(t) return(f2(t, x_input, y_input))

grad.f2_t <- function(t) {
  with(tf$GradientTape() %as% tape, {
    tape$watch(t)
    val <-f2_t(t)
  })
  return(tape$gradient(val, t))
}

r_grad_r.f2_t <- function(t) {
  t <- tf$convert_to_tensor(t)
  return(grad.f2_t(t)$numpy())
}
```

However, there is a bit of a problem when we try and use optimisers in R with these gradients.

```{r error=TRUE}
print(f2_t(t))
print(r_grad_r.f2_t(t))
print(grad(function(t) f2_t(t)$numpy(), t))

check.derivatives(.x = t, func = function(t) f2_t(t)$numpy(), func_grad = r_grad_r.f2_t)
```

So what happened here? Basically, there is a problem when switching between tensorflow's float32 type and R's double. Lets try to make everything `tf$float64`.

```{r}
f3 <- function(t, xs, ys) {
  acc <- tf$zeros_like(xs)
  for (i in 1:length(xs)) {
    mask <- tf$cast(ys < t, tf$float64)
    acc <- acc + (t - xs[i] * log(t)) * mask
  }
  return(acc %>% k_sum)
}

f3_t <- function(t) return(f3(t, tf$constant(x_input, tf$float64), tf$constant(y_input, tf$float64)))

grad.f3_t <- function(t) {
  with(tf$GradientTape() %as% tape, {
    tape$watch(t)
    val <-f3_t(t)
  })
  return(tape$gradient(val, t))
}

r_grad_r.f3_t <- function(t) {
  t <- tf$convert_to_tensor(t)
  return(grad.f3_t(t)$numpy())
}
```

Now to try again:

```{r error=TRUE}
print(f3_t(t))
print(r_grad_r.f3_t(t))
print(grad(function(t) f3_t(t)$numpy(), t))

check.derivatives(.x = t, func = function(t) f3_t(t)$numpy(), func_grad = r_grad_r.f3_t)
```

So progress, but not quite there. Lets convert in the input as well now.

```{r}
t_convert <- tf$convert_to_tensor(t, tf$float64)
print(t)
print(t_convert)

print(f3_t(t_convert))
print(r_grad_r.f3_t(t_convert))
print(grad(function(t) f3_t(tf$convert_to_tensor(t, tf$float64))$numpy(), t))

check.derivatives(.x = t, func = function(t) f3_t(tf$convert_to_tensor(t, tf$float64))$numpy(), func_grad = function(t) r_grad_r.f3_t(tf$convert_to_tensor(t, tf$float64)))
```

Its a bit of a pain, but you will need to make sure that the types work if we want to use/compare against these numeric solvers.

#### Weird Trick for Lists

Lets just change the input to a list and just try and get the tensorflow gradient.

```{r error=TRUE}
ts <- c(1, 3, 2, 2, 3)

print(f3_t(ts) %>% k_sum)
print(r_grad_r.f3_t(ts) %>% k_sum)
```

Now lets try do a rather strange transformation of `ts`.

```{r}
ts_convert <- tf$convert_to_tensor(ts, tf$float64)$numpy()
print(ts)
print(ts_convert)
print("")

print(f3_t(ts_convert) %>% k_sum)
print(r_grad_r.f3_t(ts_convert) %>% k_sum)
```

Weird right! It seems like tensorflow will remember (at least for lists) the previously converted datatype when using `tf$convert_to_tensor`.
Here are some more concrete examples with out any gradients involved.

```{r}
data <- c(1, 2, 3)

# is of type tf$float32
data %>% k_sum

# is of type tf$float32
tf$constant(c(1,2,3), tf$float64) %>% as.numeric() %>% k_sum

# is of type tf$float64
tf$constant(c(1,2,3), tf$float64)$numpy() %>% k_sum
```

This trick is kinda neat as we can now use our converted `ts_convert` as per usual in R without having to worry about converting it to a `tf$float64` explicitly.

### End to End Optimisation
TODO